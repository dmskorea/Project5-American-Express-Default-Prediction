{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMEX_colab_XGB_Baseline.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "G7Dz7osRY3UP",
        "lw2kftLqNLUz",
        "MbCBsJFAKobh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connect Google Drive"
      ],
      "metadata": {
        "id": "Pr3DQqLdNEEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCS0h21JF70z",
        "outputId": "8dc3f5f1-3b51-4ac8-f05a-49cb3dc16439"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "G7Dz7osRY3UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cd drive/MyDrive/"
      ],
      "metadata": {
        "id": "ZWr1-mwtGT84"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mkdir AMEX_COMPETITION"
      ],
      "metadata": {
        "id": "D0Q-X8zbF-eT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd AMEX_COMPETITION/"
      ],
      "metadata": {
        "id": "TaHIr4BpGknX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mkdir data"
      ],
      "metadata": {
        "id": "WbqhePwhGn1A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd data"
      ],
      "metadata": {
        "id": "DVthj5DhNnDK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd data_convert_integer_without_noise/"
      ],
      "metadata": {
        "id": "tpct4ZCZHE6F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d raddar/amex-data-integer-dtypes-parquet-format"
      ],
      "metadata": {
        "id": "_MTNRptftMc7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download & Unzip Data From Kaggle (최초 1회 실행)"
      ],
      "metadata": {
        "id": "lw2kftLqNLUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # kaggle.json 경로설정\n",
        "# !mkdir /root/.kaggle/\n",
        "\n",
        "# 필독) \n",
        "# kaggle.json은 자신의 케글 account에서 받을 수 있다. (이 단계는 각자 수행해주아야 함!!!!!)\n",
        "# 오른쪽 상단 프로필 클릭 > My account > 화면 중간쯤에 위치한 API에서 'Create New API Token'을 클릭하면 파일이 다운받아진다. \n",
        "\n",
        "#!cp [ kaggle.json 위치 ] /root/.kaggle/   # kaggl.json위치 지정\n",
        "# !cp  ../kaggle.json /root/.kaggle/\n",
        "\n",
        "#권한설정\n",
        "# !chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "n-chocS5HRv7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "phoTia86FbHg"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions download -c amex-default-prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip amex-default-prediction.zip"
      ],
      "metadata": {
        "id": "EFTUdDD7JkLE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# csv to parquet"
      ],
      "metadata": {
        "id": "MbCBsJFAKobh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyarrow.csv as pv\n",
        "# import pyarrow.parquet as pq\n",
        "\n",
        "# filename = 'train_data.csv'\n",
        "# table = pv.read_csv(filename)\n",
        "# pq.write_table(table, 'train.parquet')"
      ],
      "metadata": {
        "id": "RObH2auEKmeb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB Starter Baseline"
      ],
      "metadata": {
        "id": "wrZiBTi8NT4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install CUDF"
      ],
      "metadata": {
        "id": "8B09rGglr4Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf6MuXkhvYYM",
        "outputId": "cc416495-b31e-4c00-97d7-1253d302df03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 26 06:36:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvWr5c81w4PB",
        "outputId": "8183f38e-18d0-42aa-b64c-c57cd7d4b176"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 1.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python /content/drive/MyDrive/AMEX_COMPETITION/rapidsai-csp-utils/colab/env-check.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_UNBiTBwls9",
        "outputId": "b8e91150-9fc4-43d1-dc76-062ca0f933d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********************************************************************\n",
            "Woo! Your instance has the right kind of GPU, a Tesla P100-PCIE-16GB!\n",
            "***********************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/drive/MyDrive/AMEX_COMPETITION/rapidsai-csp-utils/colab/update_gcc.sh -qq\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyLv9yQ6wlz5",
        "outputId": "6eb6f6a2-f069-4ba6-a458-bdc1634cb307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating your Colab environment.  This will restart your kernel.  Don't Panic!\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease [20.8 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,013 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,298 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,866 kB]\n",
            "Get:19 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 Packages [50.4 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,047 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]\n",
            "Fetched 12.4 MB in 3s (4,847 kB/s)\n",
            "Reading package lists... Done\n",
            "Added repo\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Installing libstdc++\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Selected version '11.1.0-1ubuntu1~18.04.1' (Toolchain test builds:18.04/bionic [amd64]) for 'libstdc++6'\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following NEW packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following packages will be upgraded:\n",
            "  libstdc++6\n",
            "1 upgraded, 2 newly installed, 0 to remove and 64 not upgraded.\n",
            "Need to get 641 kB of archives.\n",
            "After this operation, 981 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 gcc-11-base amd64 11.1.0-1ubuntu1~18.04.1 [19.0 kB]\n",
            "Get:2 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libgcc-s1 amd64 11.1.0-1ubuntu1~18.04.1 [41.8 kB]\n",
            "Get:3 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libstdc++6 amd64 11.1.0-1ubuntu1~18.04.1 [580 kB]\n",
            "Fetched 641 kB in 1s (543 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcc-11-base:amd64.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../gcc-11-base_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Setting up gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libgcc-s1:amd64.\n",
            "(Reading database ... 155644 files and directories currently installed.)\n",
            "Preparing to unpack .../libgcc-s1_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Replacing files in old package libgcc1:amd64 (1:8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "(Reading database ... 155646 files and directories currently installed.)\n",
            "Preparing to unpack .../libstdc++6_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) over (8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "restarting Colab...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이전단계 성공 시 Colab 재시작되어 여기서부터 이후 셀 실행"
      ],
      "metadata": {
        "id": "_XcJqQ1ZbXlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNswmSStwl-n",
        "outputId": "925935da-2a7f-480f-b0ac-7a6b98370434"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:24\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0ocDkHPbb5Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15분정도 소요\n",
        "!python /content/drive/MyDrive/AMEX_COMPETITION/rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV3SRs1CwmEC",
        "outputId": "8923d0a9-54e2-44a8-b316-0224446ab25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cffi 1.14.5\n",
            "Uninstalling cffi-1.14.5:\n",
            "  Successfully uninstalled cffi-1.14.5\n",
            "Found existing installation: cryptography 3.4.5\n",
            "Uninstalling cryptography-3.4.5:\n",
            "  Successfully uninstalled cryptography-3.4.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cffi==1.15.0\n",
            "  Downloading cffi-1.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (427 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi==1.15.0) (2.20)\n",
            "Installing collected packages: cffi\n",
            "Successfully installed cffi-1.15.0\n",
            "Installing RAPIDS Stable 21.12\n",
            "Starting the RAPIDS install on Colab.  This will take about 15 minutes.\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... WARNING conda.core.solve:_add_specs(611): pinned spec cudatoolkit=11.1 conflicts with explicit specs.  Overriding pinned spec.\n",
            "failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... WARNING conda.core.solve:_add_specs(611): pinned spec cudatoolkit=11.1 conflicts with explicit specs.  Overriding pinned spec.\n",
            "failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf, cuml\n",
        "print('cuml version:', cuml.__version__, ', cudf version:', cudf.__version__)"
      ],
      "metadata": {
        "id": "fuYf7r9mwtS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD LIBRARIES\n",
        "import pandas as pd, numpy as np # CPU libraries\n",
        "import cupy, cudf # GPU libraries\n",
        "import matplotlib.pyplot as plt, gc, os\n",
        "\n",
        "print('RAPIDS version',cudf.__version__)"
      ],
      "metadata": {
        "id": "EIn4yQ91rzqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERSION NAME FOR SAVED MODEL FILES\n",
        "VER = 1\n",
        "\n",
        "# TRAIN RANDOM SEED\n",
        "SEED = 42\n",
        "\n",
        "# FILL NAN VALUE\n",
        "NAN_VALUE = -127 # will fit in int8\n",
        "\n",
        "# FOLDS PER MODEL\n",
        "FOLDS = 5"
      ],
      "metadata": {
        "id": "vTd2vAHPrztW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process and Feature Engineer Train Data"
      ],
      "metadata": {
        "id": "riLhZL9FsBwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path = '', usecols = None):\n",
        "    # LOAD DATAFRAME\n",
        "    if usecols is not None: df = cudf.read_parquet(path, columns=usecols)\n",
        "    else: df = cudf.read_parquet(path)\n",
        "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
        "    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
        "    df.S_2 = cudf.to_datetime( df.S_2 )\n",
        "    # SORT BY CUSTOMER AND DATE (so agg('last') works correctly)\n",
        "    #df = df.sort_values(['customer_ID','S_2'])\n",
        "    #df = df.reset_index(drop=True)\n",
        "    # FILL NAN\n",
        "    # df = df.fillna(NAN_VALUE) \n",
        "    print('shape of data:', df.shape)\n",
        "    \n",
        "    return df\n",
        "\n",
        "print('Reading train data...')\n",
        "TRAIN_PATH = '/content/drive/MyDrive/AMEX_COMPETITION/data_convert_integer_without_noise/train.parquet'\n",
        "train = read_file(path = TRAIN_PATH)"
      ],
      "metadata": {
        "id": "TU_wno9Lrzwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "21H0PRPNrzzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_feature_engineer(df):\n",
        "    # FEATURE ENGINEERING FROM \n",
        "    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
        "\n",
        "    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n",
        "\n",
        "    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
        "    num_features = [col for col in all_cols if col not in cat_features]\n",
        "\n",
        "    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
        "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
        "\n",
        "    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
        "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
        "\n",
        "    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n",
        "    del test_num_agg, test_cat_agg\n",
        "    print('shape after engineering', df.shape )\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "04uNGlqbnRzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = process_and_feature_engineer(train)"
      ],
      "metadata": {
        "id": "LjEnJSdxrz2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD TARGETS\n",
        "targets = cudf.read_csv('/content/drive/MyDrive/AMEX_COMPETITION/data/train_labels.csv')\n",
        "targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
        "targets = targets.set_index('customer_ID')\n",
        "train = train.merge(targets, left_index=True, right_index=True, how='left')\n",
        "train.target = train.target.astype('int8')\n",
        "del targets\n",
        "\n",
        "# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\n",
        "train = train.sort_index().reset_index()\n",
        "\n",
        "# FEATURES\n",
        "FEATURES = train.columns[1:-1]\n",
        "print(f'There are {len(FEATURES)} features!')"
      ],
      "metadata": {
        "id": "HDmAdoOxrz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train XGB"
      ],
      "metadata": {
        "id": "PWiAp3W4sKL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD XGB LIBRARY\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "print('XGB Version',xgb.__version__)\n",
        "\n",
        "# XGB MODEL PARAMETERS\n",
        "xgb_parms = { \n",
        "    'max_depth':4, \n",
        "    'learning_rate':0.05, \n",
        "    'subsample':0.8,\n",
        "    'colsample_bytree':0.6, \n",
        "    'eval_metric':'logloss',\n",
        "    'objective':'binary:logistic',\n",
        "    'tree_method':'gpu_hist',\n",
        "    'predictor':'gpu_predictor',\n",
        "    'random_state':SEED\n",
        "}"
      ],
      "metadata": {
        "id": "3XwrumRTrz8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEEDED WITH DeviceQuantileDMatrix BELOW\n",
        "class IterLoadForDMatrix(xgb.core.DataIter):\n",
        "    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.df = df\n",
        "        self.it = 0 # set iterator to 0\n",
        "        self.batch_size = batch_size\n",
        "        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n",
        "        super().__init__()\n",
        "\n",
        "    def reset(self):\n",
        "        '''Reset the iterator'''\n",
        "        self.it = 0\n",
        "\n",
        "    def next(self, input_data):\n",
        "        '''Yield next batch of data.'''\n",
        "        if self.it == self.batches:\n",
        "            return 0 # Return 0 when there's no more batch.\n",
        "        \n",
        "        a = self.it * self.batch_size\n",
        "        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n",
        "        dt = cudf.DataFrame(self.df.iloc[a:b])\n",
        "        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n",
        "        self.it += 1\n",
        "        return 1"
      ],
      "metadata": {
        "id": "tAeqZuJgrz_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/kyakovlev\n",
        "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
        "def amex_metric_mod(y_true, y_pred):\n",
        "\n",
        "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz        = cum_pos_found / total_pos\n",
        "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
        "\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)"
      ],
      "metadata": {
        "id": "sB5Nh2G_r0CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = []\n",
        "oof = []\n",
        "train = train.to_pandas() # free GPU memory\n",
        "TRAIN_SUBSAMPLE = 1.0\n",
        "gc.collect()\n",
        "\n",
        "skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "for fold,(train_idx, valid_idx) in enumerate(skf.split(\n",
        "            train, train.target )):\n",
        "    \n",
        "    # TRAIN WITH SUBSAMPLE OF TRAIN FOLD DATA\n",
        "    if TRAIN_SUBSAMPLE<1.0:\n",
        "        np.random.seed(SEED)\n",
        "        train_idx = np.random.choice(train_idx, \n",
        "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
        "        np.random.seed(None)\n",
        "    \n",
        "    print('#'*25)\n",
        "    print('### Fold',fold+1)\n",
        "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
        "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
        "    print('#'*25)\n",
        "    \n",
        "    # TRAIN, VALID, TEST FOR FOLD K\n",
        "    Xy_train = IterLoadForDMatrix(train.loc[train_idx], FEATURES, 'target')\n",
        "    X_valid = train.loc[valid_idx, FEATURES]\n",
        "    y_valid = train.loc[valid_idx, 'target']\n",
        "    \n",
        "    dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256)\n",
        "    dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n",
        "    \n",
        "    # TRAIN MODEL FOLD K\n",
        "    model = xgb.train(xgb_parms, \n",
        "                dtrain=dtrain,\n",
        "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                num_boost_round=9999,\n",
        "                early_stopping_rounds=100,\n",
        "                verbose_eval=100) \n",
        "    model.save_model(f'XGB_v{VER}_fold{fold}.xgb')\n",
        "    \n",
        "    # GET FEATURE IMPORTANCE FOR FOLD K\n",
        "    dd = model.get_score(importance_type='weight')\n",
        "    df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n",
        "    importances.append(df)\n",
        "            \n",
        "    # INFER OOF FOLD K\n",
        "    oof_preds = model.predict(dvalid)\n",
        "    acc = amex_metric_mod(y_valid.values, oof_preds)\n",
        "    print('Kaggle Metric =',acc,'\\n')\n",
        "    \n",
        "    # SAVE OOF\n",
        "    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n",
        "    df['oof_pred'] = oof_preds\n",
        "    oof.append( df )\n",
        "    \n",
        "    del dtrain, Xy_train, dd, df\n",
        "    del X_valid, y_valid, dvalid, model\n",
        "    _ = gc.collect()\n",
        "    \n",
        "print('#'*25)\n",
        "oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n",
        "acc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n",
        "print('OVERALL CV Kaggle Metric =',acc)"
      ],
      "metadata": {
        "id": "sCZa3tspr0FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN RAM\n",
        "del train\n",
        "_ = gc.collect()"
      ],
      "metadata": {
        "id": "vme3iO-qr0Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save OOF Preds"
      ],
      "metadata": {
        "id": "VEmQu3dnsWzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oof_xgb = pd.read_parquet(TRAIN_PATH, columns=['customer_ID']).drop_duplicates()\n",
        "oof_xgb['customer_ID_hash'] = oof_xgb['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
        "oof_xgb = oof_xgb.set_index('customer_ID_hash')\n",
        "oof_xgb = oof_xgb.merge(oof, left_index=True, right_index=True)\n",
        "oof_xgb = oof_xgb.sort_index().reset_index(drop=True)\n",
        "oof_xgb.to_csv(f'oof_xgb_v{VER}.csv',index=False)\n",
        "oof_xgb.head()"
      ],
      "metadata": {
        "id": "e7Z4F277r0LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT OOF PREDICTIONS\n",
        "plt.hist(oof_xgb.oof_pred.values, bins=100)\n",
        "plt.title('OOF Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M-04mVrVr0OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAR VRAM, RAM FOR INFERENCE BELOW\n",
        "del oof_xgb, oof\n",
        "_ = gc.collect()"
      ],
      "metadata": {
        "id": "BaJsXvR7r0RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance"
      ],
      "metadata": {
        "id": "nvFuUnxZsbU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = importances[0].copy()\n",
        "for k in range(1,FOLDS): df = df.merge(importances[k], on='feature', how='left')\n",
        "df['importance'] = df.iloc[:,1:].mean(axis=1)\n",
        "df = df.sort_values('importance',ascending=False)\n",
        "df.to_csv(f'xgb_feature_importance_v{VER}.csv',index=False)"
      ],
      "metadata": {
        "id": "1UpiSQLksdHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FEATURES = 20\n",
        "plt.figure(figsize=(10,5*NUM_FEATURES//10))\n",
        "plt.barh(np.arange(NUM_FEATURES,0,-1), df.importance.values[:NUM_FEATURES])\n",
        "plt.yticks(np.arange(NUM_FEATURES,0,-1), df.feature.values[:NUM_FEATURES])\n",
        "plt.title(f'XGB Feature Importance - Top {NUM_FEATURES}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OAs9ohs9sdJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULATE SIZE OF EACH SEPARATE TEST PART\n",
        "def get_rows(customers, test, NUM_PARTS = 4, verbose = ''):\n",
        "    chunk = len(customers)//NUM_PARTS\n",
        "    if verbose != '':\n",
        "        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n",
        "        print(f'There will be {chunk} customers in each part (except the last part).')\n",
        "        print('Below are number of rows in each part:')\n",
        "    rows = []\n",
        "\n",
        "    for k in range(NUM_PARTS):\n",
        "        if k==NUM_PARTS-1: cc = customers[k*chunk:]\n",
        "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
        "        s = test.loc[test.customer_ID.isin(cc)].shape[0]\n",
        "        rows.append(s)\n",
        "    if verbose != '': print( rows )\n",
        "    return rows,chunk\n",
        "\n",
        "# COMPUTE SIZE OF 4 PARTS FOR TEST DATA\n",
        "NUM_PARTS = 4\n",
        "TEST_PATH = '/content/drive/MyDrive/AMEX_COMPETITION/data_convert_integer_without_noise/test.parquet'\n",
        "\n",
        "print(f'Reading test data...')\n",
        "test = read_file(path = TEST_PATH, usecols = ['customer_ID','S_2'])\n",
        "customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
        "rows,num_cust = get_rows(customers, test[['customer_ID']], NUM_PARTS = NUM_PARTS, verbose = 'test')"
      ],
      "metadata": {
        "id": "MtNOvVlgsdMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer Test"
      ],
      "metadata": {
        "id": "_ZfEPWGnsjUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INFER TEST DATA IN PARTS\n",
        "skip_rows = 0\n",
        "skip_cust = 0\n",
        "test_preds = []\n",
        "\n",
        "for k in range(NUM_PARTS):\n",
        "    \n",
        "    # READ PART OF TEST DATA\n",
        "    print(f'\\nReading test data...')\n",
        "    test = read_file(path = TEST_PATH)\n",
        "    test = test.iloc[skip_rows:skip_rows+rows[k]]\n",
        "    skip_rows += rows[k]\n",
        "    print(f'=> Test part {k+1} has shape', test.shape )\n",
        "    \n",
        "    # PROCESS AND FEATURE ENGINEER PART OF TEST DATA\n",
        "    test = process_and_feature_engineer(test)\n",
        "    if k==NUM_PARTS-1: test = test.loc[customers[skip_cust:]]\n",
        "    else: test = test.loc[customers[skip_cust:skip_cust+num_cust]]\n",
        "    skip_cust += num_cust\n",
        "    \n",
        "    # TEST DATA FOR XGB\n",
        "    X_test = test[FEATURES]\n",
        "    dtest = xgb.DMatrix(data=X_test)\n",
        "    test = test[['P_2_mean']] # reduce memory\n",
        "    del X_test\n",
        "    gc.collect()\n",
        "\n",
        "    # INFER XGB MODELS ON TEST DATA\n",
        "    model = xgb.Booster()\n",
        "    model.load_model(f'XGB_v{VER}_fold0.xgb')\n",
        "    preds = model.predict(dtest)\n",
        "    for f in range(1,FOLDS):\n",
        "        model.load_model(f'XGB_v{VER}_fold{f}.xgb')\n",
        "        preds += model.predict(dtest)\n",
        "    preds /= FOLDS\n",
        "    test_preds.append(preds)\n",
        "\n",
        "    # CLEAN MEMORY\n",
        "    del dtest, model\n",
        "    _ = gc.collect()"
      ],
      "metadata": {
        "id": "fQ_hFy_ysdPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Submission CSV"
      ],
      "metadata": {
        "id": "6QVd8e4msrmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE SUBMISSION FILE\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test = cudf.DataFrame(index=customers,data={'prediction':test_preds})\n",
        "sub = cudf.read_csv('/content/drive/MyDrive/AMEX_COMPETITION/data/sample_submission.csv')[['customer_ID']]\n",
        "sub['customer_ID_hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
        "sub = sub.set_index('customer_ID_hash')\n",
        "sub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\n",
        "sub = sub.reset_index(drop=True)\n",
        "\n",
        "# DISPLAY PREDICTIONS\n",
        "sub.to_csv(f'/content/drive/MyDrive/AMEX_COMPETITION/result/submission_xgb_v{VER}.csv',index=False)\n",
        "print('Submission file shape is', sub.shape )\n",
        "sub.head()"
      ],
      "metadata": {
        "id": "-UX23Y_JsdR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT PREDICTIONS\n",
        "plt.hist(sub.to_pandas().prediction, bins=100)\n",
        "plt.title('Test Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zoznjkyCsdUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yvUSHA7CsdW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}